import requests
import json
import csv
import os
from time import sleep
import pandas as pd
import re

# NASA POWER API URL í…œí”Œë¦¿
API_URL = "https://power.larc.nasa.gov/api/temporal/monthly/point?parameters=ALLSKY_SFC_SW_DWN&community=RE&longitude={lon}&latitude={lat}&start=2023&end=2023&format=JSON"

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
NEIGHBOURHOODS_FILE = "Korea_Localities"
FAILED_REQUESTS_FILE = "failed_requests"
CSV_FILE = "korea_solar_data.csv"
BATCH_SIZE = 50  # ë°ì´í„° ì €ì¥ ë°°ì¹˜ í¬ê¸°

def load_existing_data(file_path):
    existing_locations = set()
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                match = re.match(r'"(.+)": \((\d+\.\d+), (\d+\.\d+)\)', line)
                if match:
                    name, lat, lon = match.groups()
                    existing_locations.add(name)
    return existing_locations

# í–‰ì •êµ¬ì—­ ë°ì´í„° ê²€ì¦ í•¨ìˆ˜
def validate_neighbourhoods(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f, start=1):
            line = line.strip()
            if not re.match(r'".+": \(\d+\.\d+, \d+\.\d+\)', line):
                print(f"âš ï¸ ì˜¤ë¥˜: {i}ë²ˆì§¸ ì¤„ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {line}")

# í–‰ì •êµ¬ì—­ ë°ì´í„°ë¥¼ íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ê¸°
def load_neighbourhoods(file_path):
    locations = {}
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                match = re.match(r'"(.+)": \((\d+\.\d+), (\d+\.\d+)\)', line)
                if match:
                    name, lat, lon = match.groups()
                    locations[name] = (float(lat), float(lon))
                else:
                    print(f"âš ï¸ ì˜ëª»ëœ ë°ì´í„° ë¬´ì‹œ: {line}")
    return locations

# ì‹¤íŒ¨í•œ ìš”ì²­ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_failed_request(location, lat, lon):
    with open(FAILED_REQUESTS_FILE, "a", encoding="utf-8") as f:
        f.write(f'"{location}": ({lat}, {lon})\n')

# íŒŒì¼ ê²€ì¦ ì‹¤í–‰
validate_neighbourhoods(NEIGHBOURHOODS_FILE)

# ëŒ€í•œë¯¼êµ­ í–‰ì •êµ¬ì—­ë³„ ìœ„ë„, ê²½ë„ ë°ì´í„°
korea_locations = load_neighbourhoods(NEIGHBOURHOODS_FILE)

# ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ì´ë¯¸ ì²˜ë¦¬ëœ ì§€ì—­ í™•ì¸)
processed_locations = load_existing_data(CSV_FILE)
failed_requests = load_existing_data(FAILED_REQUESTS_FILE)

# ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
data_list = []

# ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì²˜ë¦¬
for location, (lat, lon) in korea_locations.items():
    if location in processed_locations:
        print(f"ğŸ”„ {location} ì´ë¯¸ ì²˜ë¦¬ë¨, ê±´ë„ˆëœ€.")
        continue  # ê¸°ì¡´ ë°ì´í„°ëŠ” ê±´ë„ˆë›´ë‹¤.

    if location in failed_requests:
        print(f"âš ï¸ {location} ì´ì „ì— ì‹¤íŒ¨í•œ ìš”ì²­, ì¬ì‹œë„ ì¤‘...")

    url = API_URL.format(lat=lat, lon=lon)
    print(f"ğŸ” ìš”ì²­ URL í™•ì¸: {url}")  # ìš”ì²­ URL ì¶œë ¥í•´ì„œ í™•ì¸

    for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„
        response = requests.get(url)
        sleep(0.2)  # API ìš”ì²­ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´

        if response.status_code == 200:
            data = response.json()
            allsky_data = data.get("properties", {}).get("parameter", {}).get("ALLSKY_SFC_SW_DWN", {})

            if allsky_data:
                # 1~12ì›” ë°ì´í„°ë§Œ ì €ì¥
                filtered_data = {k: v for k, v in allsky_data.items() if k.isdigit() and int(k[-2:]) <= 12}

                row = {"Latitude": lat, "Longitude": lon, "Location": location}
                row.update(filtered_data)  # ì›”ë³„ ë°ì´í„° ì¶”ê°€
                data_list.append(row)

                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ì €ì¥
                if len(data_list) >= BATCH_SIZE:
                    with open(CSV_FILE, "a", newline="", encoding="utf-8") as csvfile:
                        fieldnames = ["Latitude", "Longitude", "Location"] + sorted(set().union(*[row.keys() for row in data_list]) - {"Latitude", "Longitude", "Location"})
                        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                        if not os.path.exists(CSV_FILE) or os.stat(CSV_FILE).st_size == 0:
                            writer.writeheader()  # ì²« ì‹¤í–‰ ì‹œ í—¤ë” ì¶”ê°€
                        writer.writerows(data_list)
                    print(f"âœ… {len(data_list)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
                    data_list = []  # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
        else:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ ({attempt+1}/3): {location} (lat: {lat}, lon: {lon})")
            if attempt == 2:
                print(f"ğŸš¨ ìµœì¢… ì‹¤íŒ¨: {location}, ì¬ì‹œë„ ëª©ë¡ì— ì¶”ê°€ë¨.")
                save_failed_request(location, lat, lon)

# ë‚¨ì€ ë°ì´í„° ì €ì¥
if data_list:
    with open(CSV_FILE, "a", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["Latitude", "Longitude", "Location"] + sorted(set().union(*[row.keys() for row in data_list]) - {"Latitude", "Longitude", "Location"})
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not os.path.exists(CSV_FILE) or os.stat(CSV_FILE).st_size == 0:
            writer.writeheader()  # ì²« ì‹¤í–‰ ì‹œ í—¤ë” ì¶”ê°€
        writer.writerows(data_list)
    print(f"âœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(data_list)}ê°œ ë°ì´í„° ì¶”ê°€ë¨.")

print(f"ğŸ¯ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {CSV_FILE}")

